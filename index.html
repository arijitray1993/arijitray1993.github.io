<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-71242572-1"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-71242572-1');
    </script>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="I am a Computer Vision Ph.D. Student at Boston University. I am excited about how to make human-AI and AI-AI teams solve tasks effectively. Consequently, I am interested in and work on models that can interact with humans using natural language, models that can rationalize and explain their decisions, and models that leverage social media to do societal good.">
    <meta name="keywords" content="arijit, arijit ray, arren, arren ray, arijit arren ray, vqa, CLIP, large langauge models, LLM, generative AI, gen AI, diffusion models, image generation, generation, deep learning, AI, AIx, AI+x, ai, computer vision, natural language processing, conversational agents, explainable ai, xai, semafor, DARPA, SRI International, Virginia Tech, SRI, VT, vt, vqa relevance, gan, question answering, nlp, cv">
    <meta name="author" content="Arijit Ray">
    <title>Arijit Ray's Webpage</title>
	
	<!-- core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="css/font-awesome.min.css" rel="stylesheet">
    <link href="css/animate.min.css" rel="stylesheet">
    <link href="css/prettyPhoto.css" rel="stylesheet">
    <link href="css/main.css" rel="stylesheet">
    <link href="css/responsive.css" rel="stylesheet">
	
	<link href="cover.css" rel="stylesheet">
	
    <!--[if lt IE 9]>
    <script src="js/html5shiv.js"></script>
    <script src="js/respond.min.js"></script>
    <![endif]-->       
 
</head><!--/head-->

<body class="homepage">

    <div class="carousel slide" style="min-height: 100%; background: url(images/network_bg.png); background-repeat: no-repeat; background-size: 100%; padding-top: 9%; padding-bottom: 12%;">        
        <div class="carousel-inner">
            <div class="container">
                <div class="row">
                    <div class="col-md-6">
                        <div class="row" style="padding-bottom: 20px;">
                            <div class="col">
                                <p><img src="images/prof_pic_s.jpg" class="img-circle" width="35%" height="12%" style="padding:10px"></p>
                                <!--<h1 class="animate" style="font-size:2.3em"><font color="#382b42">Hello World!<span class="spananimate">|</span> </font> </h1>-->
                                <h2 class="animate" style="font-size:1.6em"><font color="#382b42"> <b> Arijit Ray.</b><span class="spananimate">|</span> </font></h2>
                                <h3 class="animate" style="font-size:1.2em"><a href="http://ai.bu.edu/" style="color:#382b42;"> <b> Computer Science Ph.D. Student </a> <span class="spananimate"><font color="#382b42">|</font></span> </b> </h3>
                                <!--<h3 class="animate" style="font-size:1.2em"><a href="http://ai.bu.edu/" style="color:#382b42;"> Research Scientist Intern, Meta AI (FAIR)</a></b> <span class="spananimate"> <font color="#382b42">|</font></span> </h3> -->
                                
                                
                            </div>
                        </div>
                        <div class="row" style="padding-bottom: 2px; padding-left: 9px;">
                            <div class="col">
                                <a href="#publications" class="btn btn-primary" role="button">Projects</a>
                                <a target="_" href="RayCV.pdf" class="btn btn-primary" role="button">CV</a>   
                                <a target="_" href="https://github.com/arijitray1993" class="btn btn-primary" role="button">Github</a>  
                            </div>
                        </div>
                        <!--<div class="row" style="padding-bottom: 20px; padding-left: 9px;">
                            <div class="col">
                                <a target="_" href="aix/index.html" class="btn btn-primary" role="button">AI+x Conversations</a>
                            </div>
                        </div>-->
                        <div class="row" style="padding-bottom: 20px; padding-left: 10px;">
                            <div class="col" style="font-size: larger;">
                                <br>
                                <a target="_" href="https://scholar.google.com/citations?hl=en&user=7UgubNoAAAAJ&view_op=list_works&sortby=pubdate" role="button"><img src="https://img.icons8.com/ios/50/000000/google-scholar--v2.png" width="25"> <b>Google Scholar</b></a>   
                            </div>
                        </div>
                        <div class="row" style="padding-bottom: 20px; padding-left: 10px;">
                            <div class="col">
                                <a target="_" href="https://twitter.com/ARRay693"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" class="bi bi-twitter" viewBox="0 0 16 16">
                                    <path d="M5.026 15c6.038 0 9.341-5.003 9.341-9.334 0-.14 0-.282-.006-.422A6.685 6.685 0 0 0 16 3.542a6.658 6.658 0 0 1-1.889.518 3.301 3.301 0 0 0 1.447-1.817 6.533 6.533 0 0 1-2.087.793A3.286 3.286 0 0 0 7.875 6.03a9.325 9.325 0 0 1-6.767-3.429 3.289 3.289 0 0 0 1.018 4.382A3.323 3.323 0 0 1 .64 6.575v.045a3.288 3.288 0 0 0 2.632 3.218 3.203 3.203 0 0 1-.865.115 3.23 3.23 0 0 1-.614-.057 3.283 3.283 0 0 0 3.067 2.277A6.588 6.588 0 0 1 .78 13.58a6.32 6.32 0 0 1-.78-.045A9.344 9.344 0 0 0 5.026 15z"/>
                                </svg></a>
                                &nbsp;
                                <a target="_" href="https://www.linkedin.com/in/ray93/"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" class="bi bi-linkedin" viewBox="0 0 16 16">
                                    <path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854V1.146zm4.943 12.248V6.169H2.542v7.225h2.401zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248-.822 0-1.359.54-1.359 1.248 0 .694.521 1.248 1.327 1.248h.016zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016a5.54 5.54 0 0 1 .016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225h2.4z"/>
                                </svg></a>  
                            </div>
                        </div>
                        
                    </div>
                    <div class="col-md-6" id="about_me" >
                        <div class="row" style="padding-left: 5px;padding-right: 5px;">
                            <p class="lead" align=justify>I wish to teach machines to help humans achieve more.</h2>
                            
                                <h2>About me:</h2>
                            <p class="lead-small" align=justify>
                                I am a <a href="http://ai.bu.edu/">Computer Vision <b>Ph.D. Student</b></a>, advised by
                                <b><a href="http://ai.bu.edu/ksaenko.html">Kate Saenko</a></b> and 
                                <b><a href="http://bryanplummer.com/">Bryan Plummer</a></b> at <b><a href="https://www.bu.edu">Boston University</a></b>, and by <b><a href="http://www.ranjaykrishna.com/index.html">Ranjay Krishna</a></b> at the <b><a href="https://www.washington.edu/">University of Washington</a></b>.
                                I received my M.S from <b><a href="https://www.vt.edu/">Virginia Tech</a></b>, advised by <b><a href="https://deviparikh.com/vil.html">Devi Parikh</a></b>.  
                            </p>
                            
                            <p class="lead-small" align=justify> 
                                I am interested in advancing visual and textual reasoning in AI models. 
                                To address the difficulty in obtaining detailed reasoning annotations, my work uses simulations to teach causal interactions and reinforcement learning to discover reasoning processes that generalize to the real-world. 
                                <!-- I also enjoy creating creative ecosystems as a member of the  <a href="https://aiforimpact.github.io/">AI for Impact Venture Studio</a> at <a href="https://www.mit.edu/">MIT</a>. -->
                            </p>

                            <p class="lead-small", align=justify>If you are interested to collaborate or just chat about research on multimodal models and world models, say hi! </p>
                        </div>
                        <div class="row" style="padding-left: 5px; padding-right: 5px;">
                            <div class="col-md-6" align="center">
                                <a href="https://www.google.com/">
                                    <div style="height: 60px; display: flex; align-items: center; justify-content: center; margin-bottom: 10px;">
                                        <img src="images/google_logo.jpg" class="img-rounded" alt="google" style="max-height: 70px; max-width: 100%;">
                                    </div>
                                    <div class="caption">
                                        <p align="center" class="lead-small">Student Researcher, Google, 2025 - Present</p>
                                    </div>
                                </a>
                            </div>
                            <div class="col-md-6" align="center">
                                <a href="https://allenai.org/">
                                    <div style="height: 60px; display: flex; align-items: center; justify-content: center; margin-bottom: 10px;">
                                        <img src="images/ai2logo.png" class="img-rounded" alt="ai2" style="max-height: 40px; max-width: 100%;">
                                    </div>
                                    <div class="caption">
                                        <p align="center" class="lead-small">Student Collaborator, Allen AI (PRIOR), 2024 - 2025</p>
                                    </div>
                                </a>
                            </div>
                        </div>

                        <div class="row" style="padding-left: 5px; padding-right: 5px; margin-top: 20px;">
                            <div class="col-md-6" align="center">
                                <a href="https://x.company/">
                                    <div style="height: 60px; display: flex; align-items: center; justify-content: center; margin-bottom: 10px;">
                                        <img src="images/X_logo.png" class="img-rounded" alt="googleX" style="max-height: 50px; max-width: 100%;">
                                    </div>
                                    <div class="caption">
                                        <p align="center" class="lead-small">AI Resident, Google X Moonshot Labs, 2023</p>
                                    </div>
                                </a>
                            </div>
                            <div class="col-md-6" align="center">
                                <a href="https://ai.meta.com/research/">
                                    <div style="height: 60px; display: flex; align-items: center; justify-content: center; margin-bottom: 10px;">
                                        <img src="images/meta_ai.jpeg" class="img-rounded" alt="meta_ai_fair" style="max-height: 40px; max-width: 100%;">
                                    </div>
                                    <div class="caption">
                                        <p align="center" class="lead-small">Research Intern, Facebook AI Research, 2022</p>
                                    </div>
                                </a>
                            </div>
                        </div>
                    </div>
                        <!---
                        <div class="row" style="padding-left: 5px;padding-right: 5px;">
                            <div class="col-md-6" align="center">
                               
                                <a href="https://www.sri.com/">
                                <img src="images/sri_logo.jpeg" class="img-rounded" alt="sri" style="height:40px">
                               
                                <div class="caption">
                                    <p align="center" class="lead-small">Computer Scientist, 2017-2021</p>
                                </div>
                                </a>
                               
                            </div>
                            <div class="col-md-6" align="center">
                                
                                <a href="https://bluerivertechnology.com/">
                                <img src="images/blue_river.jpeg" class="img-rounded" alt="blueriver" style="height:40px">
                                <div class="caption">
                                    <p align="center" class="lead-small">Deep Learning Intern, 2016, acquired by John Deere</p>
                                </div>
                                </a>
                                
                            </div>
                        </div>
                        -->
                        
                        <!--<p class="lead" align=justify>    
                            I have been fortunate to work in close collaboration with amazing researchers such as
                            <a href="http://www.cc.gatech.edu/~dbatra/index.html">Prof. Dhruv Batra</a>, 
                            <a href="http://web.engr.oregonstate.edu/~leestef/">Prof. Stefan Lee</a>,
                            <a href="https://sites.google.com/view/ajaydivakaran/">Dr. Ajay Divakaran</a>, 
                            <a href="https://scholar.google.com/citations?user=iD6QaXcAAAAJ&hl=en">Dr. Yi Yao</a>, and
                            <a href="https://scholar.google.com/citations?user=KrkPjxMAAAAJ&hl=en">Dr. Giedrius Burachas</a>. 

                        </p> --> 
            
                    
                </div>
            </div>
        </div>
    </div>

    <div class="container">
        <div class="row">
            <div class="col-md-12">
                <h2 style="font-size:1.7em">Selected Highlights:</h2>
                <h3>
                    <ul>
                        <!--<li class="lead-small">2025: Our pseudo-generated <b><a href="https://abhaybd.github.io/GraspMolmo/">task to 6-DOF grasp dataset</a></b> got accepted to CORL 2025!</li>
                        <li class="lead-small">2025: Our <b><a href="https://arijitray1993.github.io/SAT/">simulated spatial reasoning training dataset</a></b> got accepted to COLM 2025!</li> -->
                        <li class="lead-small">2025: Excited to spend my summer at <b>Google</b>, working with the Pixel team and Leonidas Guibas from Deepmind.</li>
                        <li class="lead-small">2024: Our <b><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Feedback-Guided_Autonomous_Driving_CVPR_2024_paper.pdf">paper</a></b> on feedback-guided autonomous driving got accepted to CVPR 2024 as a highlight. </li>
                        <li class="lead-small">2023: One of my student's <b><a href="https://arxiv.org/abs/2308.16741">paper</a></b> on diverse reactions to multimodal content got accepted as an oral at an <a href="https://iccv23-wecia.github.io/">ICCV Workshop</a> 2023. </li>
                        <li class="lead-small">2022: Excited to be spending my summer as a Research Scientist Intern at <b>Meta (Facebook) AI</b> (FAIR), working on the compositionality of large vision-language models.</li>
                        <li class="lead-small">2022: I started a PhD; We started <b><a href="aix/index.html">[AI+X] of BU and Harvard</a></b>, where we brainstorm research/venture ideas on how AI can impact different verticals.</li>
                        <li class="lead-small">2019: I <b>won runners-up</b> at the SRI CVT Shark Tank Competition that supported my <b><a href="papers/persuade/persuade.html">mini-project on understanding image-text content</a></b> to reduce radicalization of opinions on social media.</li> 
                        <li class="lead-small">2017: The weed vs plant detection system I helped develop for precision fertilizing played a key part in the acquisition of Blue River Technologies by John Deere for <b>305 million USD</b>. </li>
                        <!-- <li class="lead-small">2016: My first <b><a href="https://arxiv.org/abs/1606.06622">paper</a></b> got accepted to EMNLP.</li>. -->
                        <li class="lead-small">2014: Our UAV to <b>help locate natural disaster victims</b> was featured in national news: <a href="https://www.newindianexpress.com/cities/chennai/2014/Sep/20/uav-with-facial-recognition-takes-flight-662823.html">Deccan Chronicle, Indian Express</a></li>
                        <!-- <li class="lead-small">2013: I <b>won</b> a <b>silver medal</b> at SRM University Research Day for my white-paper presentation on an electro-mechanical exoskeleton.</li>. -->
                        <li class="lead-small">2012: I won an <b>Academic Merit Scholarship</b> from SRM University that waives a part of my undergraduate tuition.</li>  
                    </ul>	
                </h3>
                
            </div>
        </div>
        

        <div style="padding-bottom: 40px;"></div>
        <div class="row" id="publications">
            <div class="col-md-12">
                
                <div class="publications-modern-wrapper">
                    <div class="publications-modern-section">
                        <h2 style="font-size:1.7em">Selected publications (<a target="_" href="https://scholar.google.com/citations?hl=en&user=7UgubNoAAAAJ&view_op=list_works&sortby=pubdate"><u>all</u></a>):</h2>
                        <!-- 2025 -->
                        <div class="pub-year-group">
                            <h3 class="pub-year-header">2025</h3>

                            <div class="pub-paper-card">
                                <div class="pub-paper-thumbnail">
                                    <img src="images/papers/mull_teaser.png" alt="Mull-Tokens">
                                </div>
                                <div class="pub-paper-content">
                                    <div class="pub-paper-authors">
                                        <u>Arijit Ray</u>,
                                        <a href="https://scholar.google.com/citations?user=Dr_TwpwAAAAJ&hl=en">Ahmed Abdelkader</a>,
                                        <a href="https://chengzhi-mao.github.io/">Chengzhi Mao</a>,
                                        <a href="https://bryanplummer.com/">Bryan A. Plummer</a>,
                                        <a href="http://ai.bu.edu/ksaenko.html">Kate Saenko</a>,
                                        <a href="https://ranjaykrishna.com/index.html">Ranjay Krishna</a>,
                                        <a href="https://scholar.google.com/citations?user=5JlEyTAAAAAJ&hl=en">Leonidas Guibas</a>,
                                        <a href="https://l2ior.github.io/">Wen-Sheng Chu</a>
                                    </div>
                                    <div class="pub-paper-title">Mull-Tokens: Modality-Agnostic Latent Thinking</div>
                                    <div class="pub-paper-venue"><b>arXiv 2025</b></div>
                                    <div class="pub-paper-links">
                                        <a href="https://arijitray.com/multimodal_thinking/" class="pub-paper-link">Webpage</a>
                                        <a href="https://arxiv.org/abs/2512.10941" class="pub-paper-link">arXiv</a>
                                    </div>
                                    <div class="pub-paper-tldr">
                                        <strong>TL;DR:</strong> Instead of text reasoning or explicit image thoughts, using modality-agnostic thinking tokens pretrained using multimodal thoughts is more effective for visual reasoning tasks.
                                    </div>
                                </div>
                            </div>

                            <!-- Add class="pub-featured" to any paper card to highlight it -->
                            <div class="pub-paper-card">
                                <div class="pub-paper-thumbnail">
                                    <img src="https://ellisbrown.github.io/assets/img/publication_preview/sims-v.png" alt="SIMS-V">
                                </div>
                                <div class="pub-paper-content">
                                    <div class="pub-paper-authors">
                                        <a href="https://ellisbrown.github.io/">Ellis Brown</a>,
                                        <u>Arijit Ray</u>, 
                                        <a href="https://ranjaykrishna.com/index.html">Ranjay Krishna</a>,
                                        <a href="https://www.rossgirshick.info/">Ross Girshick</a>,
                                        <a href="https://cs.nyu.edu/~fergus/">Rob Fergus</a>,
                                        <a href="https://www.sainingxie.com/">Saining Xie</a>
                                    </div>
                                    <div class="pub-paper-title">SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding</div>
                                    <div class="pub-paper-venue"><b>arXiv 2025</b></div>
                                    <div class="pub-paper-links">
                                        <a href="https://ellisbrown.github.io/sims-v/" class="pub-paper-link">Webpage</a>
                                        <a href="http://arxiv.org/abs/2511.04668" class="pub-paper-link">arXiv</a>
                                        <a href="https://huggingface.co/datasets/ellisbrown/SIMS-VSI" class="pub-paper-link">Data</a>
                                    </div>
                                    <div class="pub-paper-tldr">
                                        <strong>TL;DR:</strong> Using simulators to create spatially-rich video training data for multimodal language models. 
                                        Our 7B-parameter model fine-tuned on just 25K simulated examples outperforms the larger 72B baseline and achieves competitive performance with proprietary models on real-world spatial reasoning benchmarks.
                                    </div>
                                </div>
                            </div>

                            <div class="pub-paper-card">
                                <div class="pub-paper-thumbnail">
                                    <video muted autoplay loop style="width: 100%; height: auto;">
                                        <source src="https://abhaybd.github.io/GraspMolmo/static/videos/robot/frenchpress_pour.mp4?1754417385595" type="video/mp4">
                                    </video>
                                </div>
                                <div class="pub-paper-content">
                                    <div class="pub-paper-authors">
                                        <a href="https://abhaybd.github.io/">Abhay Deshpande</a>,
                                        <a href="https://www.linkedin.com/in/yuquand/">Yuquan Deng</a>,
                                        <u>Arijit Ray</u>,
                                        <a href="https://jordisalvador-image.blogspot.com/">Jordi Salvador</a>,
                                        <a href="https://www.semanticscholar.org/author/Winson-Han/1443358534">Winson Han</a>,
                                        <a href="https://duanjiafei.com/">Jiafei Duan</a>,
                                        <a href="https://kuohaozeng.github.io/">Kuo-Hao Zeng</a>,
                                        <a href="https://www.cs.utexas.edu/~yukez/">Yuke Zhu</a>,
                                        <a href="https://ranjaykrishna.com/">Ranjay Krishna</a>,
                                        <a href="https://rosehendrix.com/">Rose Hendrix</a>
                                    </div>
                                    <div class="pub-paper-title">GraspMolmo: Generalizable Task-Oriented Grasping via Large-Scale Synthetic Data Generation</div>
                                    <div class="pub-paper-venue"><b><a href="https://www.corl.org/">CORL 2025</a></b></div>
                                    <div class="pub-paper-links">
                                        <a href="https://abhaybd.github.io/GraspMolmo/" class="pub-paper-link">Webpage</a>
                                        <a href="https://arxiv.org/abs/2505.13441" class="pub-paper-link">arXiv</a>
                                        <a href="https://abhaybd.github.io/GraspMolmo/" class="pub-paper-link">Data</a>
                                        <a href="https://abhaybd.github.io/GraspMolmo/" class="pub-paper-link">Code</a>
                                    </div>
                                    <div class="pub-paper-tldr">
                                        <strong>TL;DR:</strong> Use textual reasoning (e.g., one should grasp "handle" of cup to drink tea) and visual reasoning 
                                        (e.g., in this image, the cup is grasped by the "handle")
                                        along with an object grasping dataset (data of 6 DOF arm positions to grasp objects) to 
                                        generate high-level task description (e.g., how should I grasp cup to drink tea?) to precise 6DOF grasp data.
                                    </div>
                                </div>
                            </div>

                            <div class="pub-paper-card">
                                <div class="pub-paper-thumbnail">
                                    <img src="papers/images/sat_teaser.png" alt="SAT">
                                </div>
                                <div class="pub-paper-content">
                                    <div class="pub-paper-authors">
                                        <u>Arijit Ray</u>,
                                        <a href="https://ellisbrown.github.io/">Ellis Brown</a>, 
                                        <a href="https://duanjiafei.com/">Jiafei Duan</a>,  
                                        <a href="https://cs-people.bu.edu/rxtan/">Reuben Tan</a>,  
                                        <a href="https://scholar.google.com/citations?user=qvUTYsUAAAAJ&hl=en">Dina Bashkirova</a>,  
                                        <a href="https://rosehendrix.com/">Rose Hendrix</a>,  
                                        <a href="https://ehsanik.github.io/">Kiana Ehsani</a>,  
                                        <a href="https://anikem.github.io/">Aniruddha Kembhavi</a>,  
                                        <a href="https://bryanplummer.com/">Bryan A. Plummer</a>,  
                                        <a href="https://ranjaykrishna.com/index.html">Ranjay Krishna</a>*,  
                                        <a href="https://kuohaozeng.github.io/">Kuo-Hao Zeng</a>*, 
                                        <a href="http://ai.bu.edu/ksaenko.html">Kate Saenko</a>*
                                    </div>
                                    <div class="pub-paper-title">SAT: Dynamic Spatial Aptitude Training for Multimodal Language Models</div>
                                    <div class="pub-paper-venue"><b><a href="https://colmweb.org/index.html">COLM 2025</a></b></div>
                                    <div class="pub-paper-links">
                                        <a href="https://arijitray1993.github.io/SAT/" class="pub-paper-link">Webpage</a>
                                        <a href="https://arxiv.org/abs/2412.07755" class="pub-paper-link">arXiv</a>
                                        <a href="https://huggingface.co/datasets/array/SAT" class="pub-paper-link">Data</a>
                                        <a href="https://github.com/arijitray1993/SAT" class="pub-paper-link">Code</a>
                                    </div>
                                    <div class="pub-paper-tldr">
                                        <strong>TL;DR:</strong> <b>Simulated</b> spatial aptitude data (SAT) using 3D physics engines involving object and camera movements can improve spatial reasoning in <b>real</b> images and videos for MLMs 
                                        while maintaining pretraining commonsense. When instruction-tuned on SAT, LLaVA-13B outperforms some larger MLMs like GPT4-V and Gemini-1.5-pro in spatial reasoning.
                                    </div>
                                </div>
                            </div>
                        </div>

                        <!-- 2024 -->
                        <div class="pub-year-group">
                            <h3 class="pub-year-header">2024</h3>

                            <div class="pub-paper-card">
                                <div class="pub-paper-thumbnail">
                                    <img src="images/papers/r2d3_teaser.png" alt="R2D3">
                                </div>
                                <div class="pub-paper-content">
                                    <div class="pub-paper-authors">
                                        <u>Arijit Ray</u>,
                                        <a href="https://scholar.google.com/citations?user=qvUTYsUAAAAJ&hl=en">Dina Bashkirova</a>,
                                        <a href="https://cs-people.bu.edu/rxtan/">Reuben Tan</a>,
                                        <a href="https://kuohaozeng.github.io/">Kuo-Hao Zeng</a>,
                                        <a href="https://bryanplummer.com/">Bryan A. Plummer</a>,
                                        <a href="https://ranjaykrishna.com/index.html">Ranjay Krishna</a>,
                                        <a href="http://ai.bu.edu/ksaenko.html">Kate Saenko</a>
                                    </div>
                                    <div class="pub-paper-title">R2D3: Imparting Spatial Reasoning by Reconstructing 3D Scenes from 2D Images</div>
                                    <div class="pub-paper-venue"><b>Tech report 2024</b></div>
                                    <div class="pub-paper-links">
                                        <a href="papers/R2D3_report.pdf" class="pub-paper-link">Report</a>
                                    </div>
                                    <div class="pub-paper-tldr">
                                        <strong>TL;DR:</strong> A visual anchor with the corresponding 3D location in text helps multimodal language models perform more accurate 3D estimation.
                                    </div>
                                </div>
                            </div>

                            <!-- Add more papers here following the same structure -->
                            <div class="pub-paper-card">
                                <div class="pub-paper-thumbnail">
                                    <img src="papers/images/fed_teaser.png" alt="FED">
                                </div>
                                <div class="pub-paper-content">
                                    <div class="pub-paper-authors">
                                        <a href="https://jimuyangz.github.io/">Jimuyang Zhang</a>,
                                        <a href="https://tzmhuang.github.io/">Zanming Huang</a>, 
                                        <u>Arijit Ray</u>, 
                                        <a href="https://eshed1.github.io/">Eshed-Ohn Bar</a>
                                    </div>
                                    <div class="pub-paper-title">FED: Feedback-Guided Autonomous Driving</div>
                                    <div class="pub-paper-venue"><b><a href="https://cvpr.thecvf.com/">CVPR 2024 (Highlight)</a></b></div>
                                    <div class="pub-paper-links">
                                        <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Feedback-Guided_Autonomous_Driving_CVPR_2024_paper.pdf" class="pub-paper-link">Paper</a>
                                    </div>
                                    <div class="pub-paper-tldr">
                                        <strong>TL;DR:</strong> MLMs can benefit autonomous driving by understanding natural language feedback and refining the next waypoint prediction.
                                    </div>
                                </div>
                            </div>

                        </div>

                        <div class="pub-year-group">
                            <h3 class="pub-year-header">2023</h3>

                            <div class="pub-paper-card">
                                <div class="pub-paper-thumbnail">
                                    <img src="images/papers/teaserfig_compositionality.png" alt="Cola">
                                </div>
                                <div class="pub-paper-content">
                                    <div class="pub-paper-authors">
                                        <u>Arijit Ray</u>, 
                                        <a href="https://filipradenovic.github.io/">Filip Radenovic</a>,
                                        <a href="https://scholar.google.co.in/citations?user=KJNUEgkAAAAJ&hl=en">Abhimanyu Dubey</a>, 
                                        <a href="https://bryanplummer.com/">Bryan Plummer</a>, 
                                        <a href="https://ranjaykrishna.com/index.html">Ranjay Krishna</a>, 
                                        <a href="http://ai.bu.edu/ksaenko.html">Kate Saenko</a>
                                    </div>
                                    <div class="pub-paper-title">Cola: A Benchmark for Compositional Text-to-image Retrieval</div>
                                    <div class="pub-paper-venue"><b><a href="https://nips.cc/">NeurIPS 2023</a></b></div>
                                    <div class="pub-paper-links">
                                        <a href="https://arxiv.org/abs/2305.03689" class="pub-paper-link">arXiv</a>
                                        <a href="https://cs-people.bu.edu/array/research/cola/" class="pub-paper-link">Project Page & Data</a>
                                    </div>
                                    <div class="pub-paper-tldr">
                                        <strong>TL;DR:</strong> Multimodal layers are the most responsible for compositional reasoning rather than the visual or textual encoders. Tuning multimodal layers over frozen representations 
                                        are more effective than tuning similar amount of parameters in the individual encoders or even the entire model.
                                    </div>
                                </div>
                            </div>
                        
                            <div class="pub-paper-card">
                                <div class="pub-paper-thumbnail">
                                    <img src="images/papers/lasagna.png" alt="Lasagna">
                                </div>
                                <div class="pub-paper-content">
                                    <div class="pub-paper-authors">
                                        <a href="https://cs-people.bu.edu/dbash/">Dina Bashkirova</a>, 
                                        <u>Arijit Ray</u>, 
                                        <a href="https://contact.georgetown.edu/view/rm2083/">Rupayan Mallick</a>, 
                                        <a href="https://bargal.georgetown.domains/">Sarah Adel Bargal</a>, 
                                        <a href="https://jimmie33.github.io/">Jianming Zhang</a>, 
                                        <a href="https://ranjaykrishna.com/index.html">Ranjay Krishna</a>, 
                                        <a href="http://ai.bu.edu/ksaenko.html">Kate Saenko</a>
                                    </div>
                                    <div class="pub-paper-title">Lasagna: Layered Score Distillation for Disentangled Object Relighting</div>
                                    <div class="pub-paper-links">
                                        <a href="https://arxiv.org/pdf/2312.00833.pdf" class="pub-paper-link">arXiv</a>
                                        <a href="https://github.com/dbash/lasagna?tab=readme-ov-file" class="pub-paper-link">Project Page & Data</a>
                                    </div>
                                    <div class="pub-paper-tldr">
                                        <strong>TL;DR:</strong> Synthetically generated examples using 3D graphics engines are effective in teaching physics-aware edits like relighting if we use score-distillation to avoid overfitting.
                                    </div>
                                </div>
                            </div>

                            <div class="pub-paper-card">
                                <div class="pub-paper-thumbnail">
                                    <img src="images/papers/teaser_emotion.png" alt="Socratis">
                                </div>
                                <div class="pub-paper-content">
                                    <div class="pub-paper-authors">
                                        <a href="https://kdeng55.github.io/website/">Katherine Deng</a>, 
                                        <u>Arijit Ray</u>, 
                                        <a href="https://cs-people.bu.edu/rxtan/">Reuben Tan</a>, 
                                        <a href="https://saadia-gabriel.github.io/">Saadia Gabriel</a>, 
                                        <a href="https://bryanplummer.com/">Bryan A. Plummer</a>, 
                                        <a href="http://ai.bu.edu/ksaenko.html">Kate Saenko</a>
                                    </div>
                                    <div class="pub-paper-title">Socratis: Are Large Multimodal Models Emotionally Aware?</div>
                                    <div class="pub-paper-venue"><b>ICCV Workshops 2023 (oral)</b>, <a href="https://iccv23-wecia.github.io/">Workshop on Emotionally and Culturally Intelligent AI</a></div>
                                    <div class="pub-paper-links">
                                        <a href="https://arxiv.org/abs/2308.16741" class="pub-paper-link">arXiv</a>
                                        <a href="https://kdeng55.github.io/socratis-website/" class="pub-paper-link">Project Page & Data</a>
                                    </div>
                                    <div class="pub-paper-tldr">
                                        <strong>TL;DR:</strong> A preliminary benchmark to test MLMs on why different people may feel different emotions for diverse reasons while viewing the same image-text content.
                                    </div>
                                </div>
                            </div>

                        </div>

                        <div class="pub-year-group">
                            <h3 class="pub-year-header">2021</h3>
                                <div class="pub-paper-card">
                                    <div class="pub-paper-thumbnail">
                                        <img src="images/papers/persuade_teaser.png" alt="User-targeted content generation">
                                    </div>
                                    <div class="pub-paper-content">
                                        <div class="pub-paper-authors">
                                            <a href="https://www.sri.com/people/ajay-divakaran/">Ajay Divakaran</a>, 
                                            <a href="https://www.ksikka.com/">Karan Sikka</a>, 
                                            <u>Arijit Ray</u>, 
                                            <a href="https://scholar.google.com/citations?user=zSIbUH4AAAAJ&hl=en">Xiao Lin</a>, 
                                            <a href="https://www.linkedin.com/in/yi-yao-b75137a/">Yi Yao</a>
                                        </div>
                                        <div class="pub-paper-title">User-targeted content generation using multimodal embeddings</div>
                                        <div class="pub-paper-venue"><b><a href="https://patentimages.storage.googleapis.com/33/12/c9/a8ba61d5bff8fc/US20210297498A1.pdf">US Patent App. 17/191,698</a></b></div>
                                        <div class="pub-paper-links">
                                            <a href="papers/persuade/persuade.html" class="pub-paper-link">Webpage</a>
                                        </div>
                                    </div>
                                </div>

                                <div class="pub-paper-card">
                                    <div class="pub-paper-thumbnail">
                                        <img src="images/papers/error_map_teaser.png" alt="VQA Error Regions">
                                    </div>
                                    <div class="pub-paper-content">
                                        <div class="pub-paper-authors">
                                            <u>Arijit Ray</u>, 
                                            <a href="https://mcogswell.io/">Michael Cogswell</a>, 
                                            <a href="https://scholar.google.com/citations?user=zSIbUH4AAAAJ&hl=en">Xiao Lin</a>, 
                                            <a href="https://www.linkedin.com/in/kamran-alipour/">Kamran Alipour</a>, 
                                            <a href="https://www.sri.com/people/ajay-divakaran/">Ajay Divakaran</a>, 
                                            <a href="https://www.linkedin.com/in/yi-yao-b75137a/">Yi Yao</a>, 
                                            <a href="https://scholar.google.com/citations?user=KrkPjxMAAAAJ&hl=en">Giedrius Burachas</a>
                                        </div>
                                        <div class="pub-paper-title">Knowing What VQA Does Not: Pointing to Error-Inducing Regions to Improve Explanation Helpfulness</div>
                                        <div class="pub-paper-venue"><b><a href="https://onlinelibrary.wiley.com/journal/26895595">Applied AI Letters (Wiley) 2021</a></b></div>
                                        <div class="pub-paper-links">
                                            <a href="https://www.authorea.com/users/422054/articles/527831-generating-and-evaluating-explanations-of-attended-and-error-inducing-input-regions-for-vqa-models" class="pub-paper-link">PDF</a>
                                            <a href="https://arxiv.org/abs/2103.14712" class="pub-paper-link">arXiv</a>
                                            <a href="https://arijitray1993.github.io/helpfulness_evaluation/" class="pub-paper-link">Project Page</a>
                                        </div>
                                    </div>
                                </div>
                        </div>

                        <div class="pub-year-group">
                            <h3 class="pub-year-header">2019</h3>
                                <div class="pub-paper-card">
                                    <div class="pub-paper-thumbnail">
                                        <img src="images/papers/consistency_vqa.png" alt="VQA Consistency">
                                    </div>
                                    <div class="pub-paper-content">
                                        <div class="pub-paper-authors">
                                            <u>Arijit Ray</u>, 
                                            <a href="https://www.ksikka.com/">Karan Sikka</a>, 
                                            <a href="https://www.sri.com/people/ajay-divakaran/">Ajay Divakaran</a>, 
                                            <a href="https://web.engr.oregonstate.edu/~leestef/">Stefan Lee</a>, 
                                            <a href="https://scholar.google.com/citations?user=KrkPjxMAAAAJ&hl=en">Giedrius Burachas</a>
                                        </div>
                                        <div class="pub-paper-title">Sunny and Dark Outside?! Improving Answer Consistency in VQA through Entailed Question Generation</div>
                                        <div class="pub-paper-venue"><b><a href="https://www.emnlp-ijcnlp2019.org/">EMNLP 2019</a></b>, also at CVPR-W 2019 VQA and Visual Dialog Workshop</div>
                                        <div class="pub-paper-links">
                                            <a href="https://arxiv.org/abs/1909.04696" class="pub-paper-link">arXiv</a>
                                            <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:ShV01ZdBlNMJ:scholar.google.com/&output=citation&scisdr=CgXjcSlzEK_QoWyqPIU:AAGBfm0AAAAAXYevJIXx9ICCQTt8kgsw5WeyrTAzWxUC&scisig=AAGBfm0AAAAAXYevJOaNssRITzak7FcGP1o_N8LZQomU&scisf=4&ct=citation&cd=-1&hl=en&scfhb=1" class="pub-paper-link">BibTeX</a>
                                            <a href="https://arijitray1993.github.io/ConVQA/" class="pub-paper-link">Data</a>
                                        </div>
                                        <div class="pub-paper-tldr">
                                            <strong>TL;DR:</strong> Use a learned teacher module to reward VQA models for predicting consistent answers to entailed questions.
                                        </div>
                                    </div>
                                </div>
                        </div>

                        <div class="pub-year-group">
                            <h3 class="pub-year-header">2016</h3>
                                <div class="pub-paper-card">
                                    <div class="pub-paper-thumbnail">
                                        <img src="images/papers/vqa_relevance.png" alt="VQA Relevance">
                                    </div>
                                    <div class="pub-paper-content">
                                        <div class="pub-paper-authors">
                                            <u>Arijit Ray</u>, 
                                            <a href="https://computing.ece.vt.edu/~gordonac/">Gordon Christie</a>, 
                                            <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a>, 
                                            <a href="https://faculty.cc.gatech.edu/~dbatra/">Dhruv Batra</a>, 
                                            <a href="https://faculty.cc.gatech.edu/~parikh/">Devi Parikh</a>
                                        </div>
                                        <div class="pub-paper-title">Question Relevance in VQA: Identifying Non-Visual And False-Premise Questions</div>
                                        <div class="pub-paper-venue"><b><a href="http://www.emnlp2016.net/">EMNLP 2016</a></b></div>
                                        <div class="pub-paper-links">
                                            <a href="https://arxiv.org/abs/1606.06622" class="pub-paper-link">PDF</a>
                                            <a href="https://github.com/arijitray1993/VQARelevance" class="pub-paper-link">Code</a>
                                            <a href="https://www.youtube.com/watch?v=wZJmHs6qSyY" class="pub-paper-link">Video</a>
                                        </div>
                                        <div class="pub-paper-tldr">
                                            <strong>TL;DR:</strong> Image captions are a surprisingly strong baseline to identify whether a question is relevant to an image or not.
                                        </div>
                                    </div>
                                </div>
                        </div>
                        <!-- Add more year groups as needed -->

                    </div>
                </div>
            </div>
        </div>
        <div style="padding-bottom: 40px;"></div>
        
        
        <!--
        <div class="row" id="tutorials">
            
                <div class="col-md-12">
                    <h2 style="font-size:1.7em"><b>Tutorials</b></h2>
                    <ul class="list-group">
                        <li class="list-group-item">
                            <div class="media">
                                <div class="pull-left">
                                    <a target="_" href="https://arijitray1993.github.io/CARLA_tutorial/">
                                        <img src="images/carla_icon.png" class="media-object" style="width:200px">
                                    </a>
                                </div>
                                <div class="media-body">
                                    <h4 class="media-heading">
                                        <a target="_" href="https://arijitray1993.github.io/CARLA_tutorial/">CARLA Tutorial</a>
                                        <a target="_" href="https://github.com/arijitray1993/CARLA_tutorial">[Github Code]</a>
                                    </h4>
                                    <p>Tutorial code on how to run CARLA without a display on an Ubuntu server and get image frames/sensor data</p>
                                </div>
                            </div>
                        </li>
                    </ul>
                </div>
           
        </div>
        -->
        <div class="row">
            <div class="features">             
                <div class="col-md-12">
                    <div class="feature-wrap">
                        <h2 style="font-size:1.7em"><b>Press Coverage</b></h2>
                        <ul>
                            <li>
                                <b>2023: Generative AI Podcast:</b> <a href="https://podcasts.apple.com/us/podcast/generative-ai-using-ai-to-predict-social-media-and/id1660801320?i=1000605725626"> I was interviewed on AI and analyzing social media responses using language models.</a>
                            </li>
                            <li>
                                <b>2019: TechXplore, Phys.org:</b> <a href="https://techxplore.com/news/2019-04-exag-image-guessing-game-machine-explanations.html">An image-guessing game to evaluate the helpfulness of machine explanations </a>
                            </li>
                            <li>
                                <b>2014: Deccan Chronicle, Indian Express, Engineering.Careers360:</b> UAV with Facial Recognition Capabilities for helping locating natural disaster victims, <a href="http://www.newindianexpress.com/cities/chennai/2014/sep/20/UAV-With-Facial-Recognition-Takes-Flight-662823.html">Click here</a>
                            </li>
                        </ul>
                    </div>
                </div><!--/.col-md-4-->
            </div>
        </div>

        <!--
        <div class="row">
            <div class="col-md-12">
                <div class="feature-wrap">
                    <h2 style="font-size:1.7em">Collaborators</h2>
                    <p class="lead">
                        Some of the amazing people I have been fortunate to work with:
                    </p>
                    <p>
                        <a href="http://www.cc.gatech.edu/~dbatra/index.html">Prof. Dhruv Batra (at Virgina Tech)</a>, 
                            <a href="http://web.engr.oregonstate.edu/~leestef/">Prof. Stefan Lee (at Virgina Tech and SRI Intl.)</a>,
                            <a href="https://scholar.google.com/citations?user=Gd9HQn2UsNoC&hl=en">Dr. Dhruv Mahajan (at FAIR)</a>,
                        <a href="https://filipradenovic.github.io/">Dr. Filip Radenovic (at FAIR)</a>,
                        <a href="https://abhimanyudubey.github.io/">Dr. Abhimanyu Dubey (at FAIR)</a>,
                            <a href="https://sites.google.com/view/ajaydivakaran/">Dr. Ajay Divakaran (at SRI Intl)</a>, 
                            <a href="https://scholar.google.com/citations?user=iD6QaXcAAAAJ&hl=en">Dr. Yi Yao (at SRI Intl)</a>,
                            <a href="https://scholar.google.com/citations?user=KrkPjxMAAAAJ&hl=en">Dr. Giedrius Burachas (at SRI Intl)</a>,
                            <a href="https://www.kezhenchen.net/">Dr. Kezhen Chen (at Google X, Mineral)</a>
                    </p>
                </div>
            </div> 
        </div>
        -->


        <div class="row">
            <div class="col-md-12">
                <div class="feature-wrap">
                    <!--<i><img width="70%" class="img-circle" src="images/hike.jpg"></img></i>-->
                    <h2 style="font-size:1.7em">Hobbies</h2>
                    <h3>When I am not training LLMs, I love going to techno (a subgenre of electronic music) fests, <a href="https://photos.app.goo.gl/Dk7KDKqC5DGYYfey9" style="text-decoration: underline" target="_blank">making latte art</a>, and <a href="https://github.com/arijitray1993/raspberrypi_surveillance" style="text-decoration: underline" target="_blank">engineering simple gadgets</a>. 
                        In middle school, I opened an informal research society to encourage fellow students to 
                        take an interest in science by constructing simple gadgets. We won multiple accolades in school and city-level exhibitions. </h3>
                </div>
            </div> 
        </div>

        

        <div class="row">
            <div class="col-md-12">
                <div class="feature-wrap">
                    <!--<i><img width="70%" class="img-circle" src="images/hike.jpg"></img></i>-->
                    <h2 style="font-size:1.7em">Miscellanea</h2>
                    <ul>
                        <li>
                            Paper writing tips and tricks: <a href="https://faculty.cc.gatech.edu/~parikh/citizenofcvpr/static/slides/malik_write_good_paper.pdf" style="text-decoration: underline">Writing a good paper (by Jitendra Mailk)</a>, 
                            <a href="https://deviparikh.substack.com/p/shortening-papers-to-fit-page-limits-97601318681d" style="text-decoration: underline">shortening papers (by Devi Parikh)</a>, 
                            <a href="https://docs.google.com/presentation/d/1PZj0Sev2yjDu9NNr96S_wwjKCgIDhGmLjW1vtQpDhlk/edit#slide=id.p" style="text-decoration: underline">Writing Introductions (by Kate Saenko)</a>
                        </li>
                        <li>
                            <a href="http://www.paulgraham.com/articles.html" style="text-decoration: underline">Paul Graham's essays:</a> Some of my favorites: <a href="http://www.paulgraham.com/newideas.html" style="text-decoration: underline">Crazy New Ideas</a>,
                             <a href="http://www.paulgraham.com/genius.html" style="text-decoration: underline">The Bus Ticket Theory of Genius</a>, <a href="http://www.paulgraham.com/think.html" style="text-decoration: underline">How to Think for Yourself</a>, 
                             <a href="https://paulgraham.com/cities.html" style="text-decoration: underline">Cities and Ambition</a>
                            
                        </li>
                        <li>
                            On social media: Is it bad or beneficial to society, and why running a social media is hard: <a href="https://twitter.com/yishan/status/1514938507407421440" style="text-decoration: underline">Twitter thread</a> by Yishan (former CEO of Reddit).
                        </li>
                        <li>
                            <a href="miscc/quotes.html" style="text-decoration: underline">Quotes</a>
                        </li>
                    </ul>
                </div>
            </div> 
        </div>


    </div>

    <div style="padding-bottom: 30px;"></div>
    <div class="container">
        <div class="row">
            <div class="col-sm-12">
                <div class="media contact-info">
                    <div class="media-body">
                        <h2><b><u>Contact</u></b></h2>
                    </div>
                    <div class="media-body">
                        <p>Best way to reach me would be to drop an email to array at bu dot edu. </p>
                    </div>
                </div>
            </div>
        </div>
    </div>

    
    <footer id="footer" class="midnight-blue">
        <div class="container">
            <div class="row">		               
                    &copy; 2024 Arijit Ray. Template modified by permission from shapebootstrap.com            
            </div>
        </div>
    </footer>

   

    <script src="js/jquery.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/jquery.prettyPhoto.js"></script>
    <script src="js/jquery.isotope.min.js"></script>
    <script src="js/main.js"></script>
    <script src="js/wow.min.js"></script>
    
</body>
</html>


<!--<div class="panel panel-default">
                            <div class="panel-heading">
                              <h3 class="panel-title">
                                <a class="accordion-toggle" data-toggle="collapse" data-parent="#accordion1" href="#collapseTwo1">
                                  Selected Presentations:
                                  <i class="fa fa-angle-right pull-right"></i>
                                </a>
                              </h3>
                            </div>
                           <div id="collapseTwo1" class="panel-collapse collapse">
                            <div class="panel-body">
                                <div class="media accordion-inner">
                                        <div class="media accordion-inner">
                                            <div class="media-body">
						                        <p> <a target="_blank" href="https://docs.google.com/presentation/d/1Au4k_-uoP3m6Ng6xFlMwziHhWuf4Yv9TNCu6Ndr7Dpk/pub?start=false&loop=false&delayms=60000"> People, Faces, and more...</a></p>
					                         </div>
                                            <div class="media-body">
						                        <p> <a target="_blank" href="https://docs.google.com/presentation/d/1WuXm45OeK5WdRN9pej9c-0-xSj5JEAXt1uGHAgBx9Rc/pub?start=false&loop=false&delayms=30000"> MACV Talk on the Visual Twenty Questions Game </a></p>
					                         </div>

                                        </div>
                               </div>
                            </div>
                          </div>
                        </div>
-->
